Neural Network Basics:
- Binary Classification
    - Example:
        - Does image contain a cat?
    - Notation for such problem:
        - M is number of training vectors
        - Nx is size of input vector
        - Ny is size of output vector
        - X(1) is first input vector
        - Y(1) is first output vector 
        - X = [X(1) X(2)..X(M)]
        - Y = [Y(1) Y(2)..Y(M)]
- Logistic Regression
    - Used for supervised learning with output of 0 or 1
    - Given x, what Y-hat = Probability (y=1|X)
    - X = real vector of nx vector
    - Parameters: w=real vector of nx and b = R
    - so output = Y-hat = sigmoid(W Transpose X + b)
    - The way sigmoid works:
        - sigmoid(x) = 1/(1+e^-x)
            - Large positive number = 1/1 = 1
            - Large negative number = 1/(1+BIG) = 0
    -Alternate Notation 
        - Y-hat = sigmoid(theta transpose x)
        - theta = [theta 0 (b) (theta one, theta two, theta three ...)(w)] 
        - NOTE we will NOT use this
- Logistic Regression Cost Function:
    - Given {(X1, Y1), ... , (Xm, Ym)}, want Y-hat i is close to actual Yi
    - Loss function to measure accuracy of one value
        - Gradient Descent doesnt really work for log Regression
        - What we actual use
            - Loss(y-hat, y) = -(ylog(y-hat) + (1-y)log(1-y-hat))
                - if y = 1, Loss = -log(y-hat), meaning you want y-hat to be large (aka, one)
                - if y = 0, loss = -log(1-y-hat), opposite so y-hat small (aka zero)
    - Cost Function - checks for entire training set 
        - Average of all loss values = -1/m (sum from i = 1 to M) (Loss(y-hat, y))
    - So we want to min max cost function

- Gradient Descent
    - Minimize to global descent of cost function
        - lets call cost function as j(w)
        - W = W - alpha (learning rate) (derivative of J(w))
            - We repeat this until the derivative is zero
            - this means the local minimum has been found
            - Kind of like euler method
- Computation Graph
    - Example: Function J(a, b, c) = 3(a + bc)
        - so u = bc, v = a+u, and J = 3v
        so steps of forward propagation form the Computation Graph
        Each step backwards from output variable shows the derivative of it

- Derivatives with Computation Graph
    - lets say that a = 5, b = 3, c = 2
    - if we increase v by 0.001, j increases by 0.003 so
    - derivative d(J)/d(v) = 3

    - Next example: change a by 0.001
    - V increases by 0.001
    - J increases by 0.003
    - So derivative of dJ/da = 3
    so, this is just chain rule

Logistic Regression Gradient Descent:
- z = wTx + b 
- y-hat = a = sigmoid(z)
- Loss(a, y) = -(ylog(x)+(1-y)log(1-a))

Computation Graph with only two features:

X1 \
W1  \
X2   ==> z = w1x1+w2x2+b ==> a = sigmoid(z) ==> L(a,y)
W2  /   dz=dL/dz=a-y         da = dL/da = -y/a + (1-y)/(1-a)
B  /

Logistic Regression on m examples

We have these variables
	X1                  Feature
	X2                  Feature
	W1                  Weight of the first feature.
	W2                  Weight of the second feature.
	B                   Logistic Regression parameter.
	M                   Number of training examples
	Y(i)                Expected output of i

which makes 
X1 \
W1  \
X2   ==> z = w1x1+w2x2+b ==> a = sigmoid(z) ==> L(a,y)
W2  /   
B  /

from left to right, we can calculate the derivatives 
	d(a)  = d(l)/d(a) = -(y/a) + ((1-y)/(1-a))
	d(z)  = d(l)/d(z) = a - y
	d(W1) = X1 * d(z)
	d(W2) = X2 * d(z)
	d(B)  = d(z)

Since cost is the average of losses, the derivative of cost
is the average of all losses
 - This will give us the descent

 J = 0, dw1 = 0, dw2 = 0, db = 0

for i = 1 to M
    zi = wTxi +B 
    ai = sigmoid(z(i))
    dzi = ai-Yi
    dw1 += x1(i) *dz(i) ... for n features
    db += dz(i)

J /= m, dw1/=m, dw2/=m, db/=m as all are to be average

so each time we have a change of -(learning rate) * derivative


Vectorization:
- w = [...] and x = [....]
- so we transposed (dot product) them for z
- vectorized implementation:
    - x = np.dot(w, x) + b

- Neural Network Guideleines:
    - Avoid explicit for-loops
        - vector u as the product of A*vector V
        - non vectored solution:
        for i ..
            for j ...
                u[i] += a[i][j]*v[j]
        - vectored solution:
            u = np.dot(a,v)
    - Another example:
        - want to apply exp function on every element
            - u = np.zeroes((n,1))
            - for i in range(n):
                u[i] = math.exp(v[i])
            But vectored solution:
                np.exp(v)
                    other functions as well
                        np.log(v)
                        np.ans(v)
                        np.max(v,0)
                        v**2
                        1/V

we had two for loops gradient descent
    - first lets remove dw1, and dw2 instead make;
        - dw = np.zeroes((n-x, 1))
        - so then we would have dw += xi * dzi
    - Same can be done with the outisde loop as well
        - However if we want multiple gradient descent 
            we still need a for loop for each epoch
